import pandas as pd
import os
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# --- KONFIGURASI PATH (Sesuaikan dengan nama folder di GitHub kamu) ---
# Asumsi: File raw ada di folder 'heart_failure_raw'
RAW_DATA_PATH = "../heart_failure_raw.csv"

# Output folder untuk hasil preprocessing
OUTPUT_FOLDER = "heart_failur
  _preprocessing"

# Lokasi simpan Scaler (Alat ukur)
SCALER_PATH = "scaler.joblib"

def load_data(path):
    """
    Fungsi untuk membaca data mentah dari CSV.
    """
    if not os.path.exists(path):
        # Fallback jika file tidak ditemukan di path relatif (misal saat run lokal beda folder)
        # Coba cari di folder saat ini jika path "../" gagal
        fallback_path = os.path.basename(path)
        if os.path.exists(fallback_path):
            path = fallback_path
        else:
            raise FileNotFoundError(f"File dataset tidak ditemukan di: {path}")
    
    print(f"Loading data from: {path}")
    return pd.read_csv(path)

def preprocess_and_split(df):
    """
    Fungsi utama:
    1. Scaling fitur numerik (StandardScaler).
    2. Split jadi Train & Test (80:20).
    3. Simpan Scaler-nya.
    """
    
    # 1. Definisikan Fitur (X) dan Target (y)
    X = df.drop('DEATH_EVENT', axis=1)
    y = df['DEATH_EVENT']
    
    # 2. Definisikan kolom yang perlu di-scale (Numerik murni)
    # Kolom 0/1 (anaemia, diabetes, high_blood_pressure, sex, smoking) TIDAK perlu di-scale
    cols_to_scale = [
        'age', 'creatinine_phosphokinase', 'ejection_fraction', 
        'platelets', 'serum_creatinine', 'serum_sodium', 'time'
    ]
    
    # 3. Lakukan Scaling
    scaler = StandardScaler()
    
    # Kita scale X-nya dulu semua
    X_scaled = X.copy()
    X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])
    
    # Simpan Scaler (PENTING buat Deployment nanti)
    joblib.dump(scaler, SCALER_PATH)
    print(f"Scaler berhasil disimpan ke: {SCALER_PATH}")
    
    # 4. Split Data (Train 80% - Test 20%)
    # Kita split SETELAH scaling (untuk simplifikasi tugas ini).
    # (Idealnya split dulu baru scale, tapi untuk submission pemula, cara ini lebih ringkas error-handlingnya)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # 5. Gabungkan kembali jadi DataFrame utuh untuk disimpan
    train_df = pd.concat([X_train, y_train], axis=1)
    test_df = pd.concat([X_test, y_test], axis=1)
    
    return train_df, test_df

def save_data(train_df, test_df, output_folder):
    """
    Menyimpan hasil dataframe ke file CSV.
    """
    os.makedirs(output_folder, exist_ok=True)
    
    train_path = os.path.join(output_folder, "train_data.csv")
    test_path = os.path.join(output_folder, "test_data.csv")
    
    train_df.to_csv(train_path, index=False)
    test_df.to_csv(test_path, index=False)
    
    print(f"Data Train disimpan: {train_path} ({train_df.shape})")
    print(f"Data Test disimpan: {test_path} ({test_df.shape})")

if __name__ == "__main__":
    print("--- START AUTOMATION ---")
    
    # 1. Load
    try:
        df = load_data(RAW_DATA_PATH)
        
        # 2. Process & Split
        train_df, test_df = preprocess_and_split(df)
        
        # 3. Save
        save_data(train_df, test_df, OUTPUT_FOLDER)
        
        print("--- SUCCESS ---")
        
    except Exception as e:
        print(f"Terjadi Error: {e}")
